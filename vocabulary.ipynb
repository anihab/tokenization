{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bYqWNp2GxcOl"
      ],
      "authorship_tag": "ABX9TyNizfBZCS8EOAbwH2i/bgxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/tokenization/blob/main/vocabulary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "bYBKMF2UBuFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from Bio import SeqIO\n",
        "from tokenizers import Tokenizer, models, trainers, normalizers"
      ],
      "metadata": {
        "id": "DvTSAkKQIeDh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a BPE tokenizer on the full bacterial training directory to create the vocabulary file."
      ],
      "metadata": {
        "id": "a_pIqlUSHKOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 0\n",
        "\n",
        "define vocabulary size and input data path\n",
        "- input should be a path to a directory of bacterial fasta files"
      ],
      "metadata": {
        "id": "xKlq4YKA18Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Globals\n",
        "VOCAB_SIZE=4096\n",
        "INPUT_PATH='/ocean/projects/bio230026p/lindseyl/DATA/SEGMENTS/bacteria'"
      ],
      "metadata": {
        "id": "A4o0BR8HOek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1\n",
        "\n",
        "parse through all fasta files in full bacterial training directory\n",
        "- to make input similar to input for pre-training, use sequences of at least 1500 nt (i.e., min sequence length = 1500)\n",
        "- do not limit max sequence length\n",
        "\n",
        "###Step 2\n",
        "\n",
        "build vocabulary json - limit vocab size to 4096"
      ],
      "metadata": {
        "id": "dQgmvQRhHbvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_iterator():\n",
        "  '''\\\n",
        "  iterates through the sequences of every csv file in a given directory\n",
        "  '''\n",
        "  if os.path.isdir(INPUT_PATH):\n",
        "        for filename in os.listdir(INPUT_PATH):\n",
        "          f = os.path.join(INPUT_PATH, filename)\n",
        "          with open(f, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "              # ensure the correct number of columns exists\n",
        "              if len(row) > 4:\n",
        "                seq = row[4]\n",
        "                # min sequence length = 1500\n",
        "                if len(seq) >= 1500:\n",
        "                  yield seq\n",
        "\n",
        "def build_vocab():\n",
        "  '''\\\n",
        "  builds a vocabulary on input\n",
        "  '''\n",
        "  tokenizer = Tokenizer(models.BPE())\n",
        "  tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
        "  trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE)\n",
        "  tokenizer.train_from_iterator(sequence_iterator(), trainer=trainer)\n",
        "  tokenizer.save(\"my_vocabulary.json\")"
      ],
      "metadata": {
        "id": "9dUis8uewwx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_vocab()"
      ],
      "metadata": {
        "id": "LUPTDqiNw0On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OLD"
      ],
      "metadata": {
        "id": "bYqWNp2GxcOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sequences(input_path):\n",
        "  '''\n",
        "  input: a directory of fasta files or a .txt file that has a list of fasta files\n",
        "  output: a list of sequences\n",
        "  '''\n",
        "  sequences = [] # the only information we need is the sequences, we do not need to limit length\n",
        "  if os.path.isdir(input_path):\n",
        "    for filename in os.listdir(input_path):\n",
        "      f = os.path.join(input_path, filename)\n",
        "      if os.path.isfile(f):\n",
        "        if f.endswith('.gz'):\n",
        "          f = gzip.open(f, 'rt', encoding='utf-8')\n",
        "        for record in SeqIO.parse(f, 'fasta'):\n",
        "          seq = str(record.seq).upper() # min sequence length = 1500\n",
        "          if len(seq) >= 1500:\n",
        "            sequences.append(seq)\n",
        "  return sequences\n",
        "\n",
        "def build_vocab(sequences):\n",
        "  '''\n",
        "  input: a list of sequences\n",
        "  output: a vocabulary json file\n",
        "  '''\n",
        "  tokenizer = Tokenizer(models.BPE())\n",
        "  # customize the tokenizer to handle DNA sequences\n",
        "  tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
        "  # train the tokenizer on DNA sequences\n",
        "  trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE)\n",
        "  tokenizer.train_from_iterator(sequences, trainer=trainer)\n",
        "  tokenizer.save(\"my_vocabulary.json\")\n",
        "\n",
        "# build vocabulary\n",
        "sequences = parse_sequences(INPUT_PATH)\n",
        "build_vocab(sequences)"
      ],
      "metadata": {
        "id": "wdbo0a1gxbkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}