{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnOG27DSWqR4On9l8LmWBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/tokenization/blob/main/vocabulary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install Bio\n",
        "!pip install tokenizers\n",
        "from Bio import SeqIO\n",
        "from tokenizers import Tokenizer, models, trainers, normalizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvTSAkKQIeDh",
        "outputId": "254e57eb-2171-4848-8ee3-59c3ae8e538e",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.6.2-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.1)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.8.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.23.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.6.2 biopython-1.83 biothings-client-0.3.1 gprofiler-official-1.0.0 mygene-3.2.2\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MhzJo_8P1z79",
        "outputId": "dba9bf33-530d-4cd4-c650-f9e0065d976c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a BPE tokenizer on the full bacterial training directory to create the vocabulary file."
      ],
      "metadata": {
        "id": "a_pIqlUSHKOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 0\n",
        "\n",
        "define vocabulary size and input data path\n",
        "- input should be a path to a directory of bacterial fasta files"
      ],
      "metadata": {
        "id": "xKlq4YKA18Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Globals\n",
        "VOCAB_SIZE = 4096\n",
        "\n",
        "# Input\n",
        "INPUT_PATH = \"/ocean/projects/bio230026p/lindseyl/DATA/SEGMENTS/bacteria\""
      ],
      "metadata": {
        "id": "A4o0BR8HOek0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1\n",
        "\n",
        "parse through all fasta files in full bacterial training directory\n",
        "- to make input similar to input for pre-training, use sequences of at least 1500 nt (i.e., min sequence length = 1500)\n",
        "- do not limit max sequence length"
      ],
      "metadata": {
        "id": "dQgmvQRhHbvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a4lGEL_rHG3M"
      },
      "outputs": [],
      "source": [
        "def parse_sequences(input_path):\n",
        "  '''\n",
        "  input: a directory of fasta files or a .txt file that has a list of fasta files\n",
        "  output: a list of sequences\n",
        "  '''\n",
        "  sequences = [] # the only information we need is the sequences, we do not need to limit length\n",
        "  if os.path.isdir(input_path):\n",
        "    for filename in os.listdir(input_path):\n",
        "      f = os.path.join(input_path, filename)\n",
        "      if os.path.isfile(f):\n",
        "        if f.endswith('.gz'):\n",
        "          f = gzip.open(f, 'rt', encoding='utf-8')\n",
        "        for record in SeqIO.parse(f, 'fasta'):\n",
        "          seq = str(record.seq).upper() # min sequence length = 1500\n",
        "          if len(seq) >= 1500:\n",
        "            sequences.append(seq)\n",
        "  return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2\n",
        "\n",
        "build vocabulary json - limit vocab size to 4096"
      ],
      "metadata": {
        "id": "diuie48HH5MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sequences):\n",
        "  '''\n",
        "  input: a list of sequences\n",
        "  output: a vocabulary json file\n",
        "  '''\n",
        "  tokenizer = Tokenizer(models.BPE())\n",
        "  # customize the tokenizer to handle DNA sequences\n",
        "  tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
        "  # train the tokenizer on DNA sequences\n",
        "  trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE)\n",
        "  tokenizer.train_from_iterator(sequences, trainer=trainer)\n",
        "  tokenizer.save(\"my_vocabulary.json\")"
      ],
      "metadata": {
        "id": "zXTRvhSUI16R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3\n",
        "build vocabulary on input"
      ],
      "metadata": {
        "id": "5wC8q_et1Oto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocabulary\n",
        "sequences = parse_sequences(INPUT_PATH)\n",
        "build_vocab(sequences)"
      ],
      "metadata": {
        "id": "U2VnLHZg1Rho"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}