{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/tokenization/blob/main/statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib_venn import venn2\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "jaj21X7F8Y4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenization Statistics**\n",
        "\n",
        "A collection of functions and figures to analyze tokenized output."
      ],
      "metadata": {
        "id": "6Hbwe7t43ejA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Calculate Subword Fertility**\n",
        "\n",
        "This is normally defined as the average number of subwords produced per tokenized word. Since we don't have tokenized words, we will instead calculate the average number of subwords per input sequence (which is currently 500 nt)."
      ],
      "metadata": {
        "id": "rSuE-ZgRWoK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMfh8hfqWX-t"
      },
      "outputs": [],
      "source": [
        "# given a single tokenized csv file, calculate the subword fertility\n",
        "# all of the tokens for a single input sequence are on a single line of the csv, separated by whitespace. the tokens for the next input sequence are on the next line.\n",
        "def subword_fertility(filepath):\n",
        "  total_subwords = 0\n",
        "  total_words = 0\n",
        "\n",
        "  with open(filepath, 'r') as file:\n",
        "      for row in csv.reader(file):\n",
        "          if row:\n",
        "              subwords = row[0].split()\n",
        "              total_subwords += len(subwords)\n",
        "              total_words += 1\n",
        "\n",
        "  if total_words > 0:\n",
        "      return total_subwords / total_words # avg number of tokens per sequence\n",
        "  else:\n",
        "      return 0\n",
        "\n",
        "# given a directory of tokenized files, calculate the average subword fertility for all files\n",
        "def subword_fertility_dir(directory):\n",
        "  total_avg = 0\n",
        "  file_count = 0\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "      if filename.endswith('.csv'):\n",
        "          filepath = os.path.join(directory, filename)\n",
        "          avg = subword_fertility(filepath)\n",
        "          total_avg += avg\n",
        "          file_count += 1\n",
        "\n",
        "  if file_count > 0:\n",
        "      return total_avg / file_count\n",
        "  else:\n",
        "      return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Calculate the Max, Min and Average token length from tokenized files**\n",
        "\n",
        "additionally produce a *histogram* of token lengths."
      ],
      "metadata": {
        "id": "5chskwfk__BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given a single tokenized csv file, calculate and return a tuple of the max, min, and average token lengths.\n",
        "def token_stats(filepath):\n",
        "  lengths = [] # will hold the lengths of each token from a single input sequence\n",
        "\n",
        "  with open(filepath, 'r') as file:\n",
        "      for row in csv.reader(file):\n",
        "          if row:\n",
        "              tokens = row[0].split()\n",
        "              lengths.extend(len(token) for token in tokens)\n",
        "\n",
        "  if lengths:\n",
        "      max_len = max(lengths)\n",
        "      min_len = min(lengths)\n",
        "      avg_len = sum(lengths) / len(lengths)\n",
        "      return max_len, min_len, avg_len\n",
        "  else:\n",
        "      return 0, 0, 0\n",
        "\n",
        "# given a directory of tokenized files, calculate the max, min, and average token lengths for all files.\n",
        "def token_stats_dir(directory):\n",
        "    max_lengths = []\n",
        "    min_lengths = []\n",
        "    avg_lengths = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            max_len, min_len, avg_len = token_stats(filepath)\n",
        "            max_lengths.append(max_len)\n",
        "            min_lengths.append(min_len)\n",
        "            avg_lengths.append(avg_len)\n",
        "\n",
        "    if max_lengths:\n",
        "        max_len = max(max_lengths)\n",
        "        min_len = min(min_lengths)\n",
        "        avg_len = sum(avg_lengths) / len(avg_lengths)\n",
        "        return max_len, min_len, avg_len\n",
        "    else:\n",
        "        return 0, 0, 0\n",
        "\n",
        "# given a directory of tokenized files, plot a histogram of the token lengths\n",
        "def length_histogram(directory):\n",
        "  lengths = []\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'r') as file:\n",
        "                for row in csv.reader(file):\n",
        "                    if row:\n",
        "                        tokens = row[0].split()\n",
        "                        lengths.extend(len(token) for token in tokens)\n",
        "\n",
        "  if lengths:\n",
        "        plt.hist(lengths, range=(min(lengths), max(lengths)), color='skyblue', edgecolor='black')\n",
        "        plt.xlabel('Token Length')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Histogram of Token Lengths')\n",
        "        plt.show()\n",
        "  else:\n",
        "        print(\"No CSV files found in the directory.\")"
      ],
      "metadata": {
        "id": "J3p9sRc8BV9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Calculate the Max, Min and Average token length from corpus**\n",
        "\n",
        "additionally produce a *histogram* of token lengths."
      ],
      "metadata": {
        "id": "T3dpjSeJNLpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given a json file, extract the vocabulary\n",
        "def get_vocab(filepath):\n",
        "  with open(filepath, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    corpus = data[\"model\"][\"vocab\"]\n",
        "    return corpus.keys()\n",
        "\n",
        "# given a json vocabulary file, calculate the max, min, and average token length\n",
        "# additionally, produce a histogram of the token lengths\n",
        "def corpus_stats(filepath):\n",
        "  lengths = []\n",
        "  vocab = get_vocab(filepath)\n",
        "\n",
        "  if vocab:\n",
        "    lengths.extend(len(token) for token in vocab)\n",
        "\n",
        "  if lengths:\n",
        "    max_len = max(lengths)\n",
        "    min_len = min(lengths)\n",
        "    avg_len = sum(lengths) / len(vocab)\n",
        "\n",
        "    plt.hist(lengths, range=(min_len, max_len), color='skyblue', edgecolor='black')\n",
        "    plt.xlabel('Token Length')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Token Lengths')\n",
        "    plt.show()\n",
        "\n",
        "    return max_len, min_len, avg_len\n",
        "  else:\n",
        "    return 0, 0, 0"
      ],
      "metadata": {
        "id": "vYQ-ZNVQNWFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Calculate Coverage Metrics**\n",
        "\n",
        "Returns the number of unused words in a corpus and additionally produces a *histogram* of used word frequencies."
      ],
      "metadata": {
        "id": "GfkzG3Dt-4JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given a json vocabulary file and a directory of tokenized csv files, calculate the number of unused words\n",
        "# additionally produce a histogram of used word frequencies\n",
        "def coverage_stats(jsonfile, directory):\n",
        "  unused = []\n",
        "  used = []\n",
        "  vocab = get_vocab(jsonfile)\n",
        "\n",
        "  if not vocab:\n",
        "    return \"json file error\"\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'r') as file:\n",
        "                for row in csv.reader(file):\n",
        "                    if row:\n",
        "                        used.extend(token for token in row[0].split())\n",
        "\n",
        "  if used:\n",
        "    for token in vocab:\n",
        "        if token not in used:\n",
        "            unused.append(token)\n",
        "\n",
        "    tokens, frequencies = zip(*Counter(used).items())\n",
        "    plt.bar(tokens, frequencies)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Word Frequencies')\n",
        "    plt.show()\n",
        "\n",
        "    plt.pie(frequencies, labels=tokens, autopct='%1.1f%%')\n",
        "    plt.axis('equal')\n",
        "    plt.title('Pie Chart of Word Frequencies')\n",
        "    plt.show()\n",
        "    return \"Unused words: \" + str(len(unused))\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "GHGa9UJCM55U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vocabulary Comparison**\n",
        "\n",
        "produce a venn diagram to compare different vocabulary outputs"
      ],
      "metadata": {
        "id": "byNYlZgp4QrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_comparison(filepath1, filepath2, name1, name2):\n",
        "  vocab1 = get_vocab(filepath1)\n",
        "  vocab2 = get_vocab(filepath2)\n",
        "  if vocab1 and vocab2:\n",
        "    venn2([set(vocab1), set(vocab2)], (name1, name2))\n",
        "    plt.title('Vocabulary Venn Diagram')\n",
        "    plt.show()\n",
        "  else:\n",
        "    return \"json file error\""
      ],
      "metadata": {
        "id": "g_nJHyec4Yyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Main**"
      ],
      "metadata": {
        "id": "HqA8d0zZNDjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"subword fertility: \" + str(subword_fertility_dir(\"/content\")))\n",
        "print(\"max, min, and avg token lengths: \" + str(token_stats_dir(\"/content\")))\n",
        "length_histogram(\"/content\")\n",
        "corpus_stats(\"/content/tokenizer.json\")\n",
        "coverage_stats(\"/content/dna_tokenizer.json\", \"/content\")\n",
        "vocab_comparison(\"/content/dna_tokenizer.json\", \"/content/tokenizer.json\", \"Bacteria Vocab\", \"DNABERT2 Vocab\")"
      ],
      "metadata": {
        "id": "T3cWb2CZ4rA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}