{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/tokenization/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1vzDUnVBe3Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bf025b-ebd9-4aa0-9f50-2cb7667e2a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.81)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Install Biopython \n",
        "try:\n",
        "    import google.colab\n",
        "    # Running on Google Colab, so install Biopython first\n",
        "    !pip install biopython\n",
        "    # for byte pair encoding\n",
        "    !pip install tokenizers\n",
        "    !pip install transformers\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkZE4bpvxM2E",
        "outputId": "be0327c8-a402-4a35-871b-9222e2f2407f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resources:\n",
        "# https://www.tutorialspoint.com/biopython/biopython_sequence_io_operations.htm\n",
        "# https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n",
        "# https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\n",
        "# https://huggingface.co/docs/transformers/model_doc/roberta\n",
        "# https://huggingface.co/docs/tokenizers/pipeline\n",
        "# https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface"
      ],
      "metadata": {
        "id": "8nfx8YAcxSll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from Bio import SeqIO\n",
        "from google.colab import files\n",
        "\n",
        "MAX_TOKENS = 510\n",
        "\n",
        "## Given a phage directory and a bacteria directory, tokenize all fasta files according to method of choice \n",
        "\n",
        "def read_files(phage_dir, bacteria_dir, method, *args, **kwargs):\n",
        "  k = kwargs.get('k', None)\n",
        "  for filename in os.listdir(phage_dir):\n",
        "    f = os.path.join(phage_dir, filename)\n",
        "    if os.path.isfile(f):\n",
        "      tokenize(f, 1, method, k)\n",
        "  for filename in os.listdir(bacteria_dir):\n",
        "    f = os.path.join(bacteria_dir, filename)\n",
        "    if os.path.isfile(f):\n",
        "      tokenize(f, 0, method, k)\n",
        "\n",
        "## Tokenizes a sequence given a fasta file and max length\n",
        "\n",
        "\"\"\"\\\n",
        "Runs fasta files through tokenizer and adds the label of 1 for phage and\n",
        "0 for bacteria. Then shuffles the rows in the dataframe and saves to CSV \n",
        "\n",
        "Input:\n",
        "  phage -- str, path to phage fasta file\n",
        "  bacteria -- str, path to bacteria fasta file\n",
        "  method -- str, tokenization method of choice\n",
        "  k -- int, length of k if using kmer tokenization\n",
        "\"\"\"\n",
        "def tokenize(filepath, label, method, *args, **kwargs):\n",
        "  sequences = []\n",
        "  tokens = []\n",
        "  \n",
        "  k = kwargs.get('k', None)\n",
        "  filename = os.path.basename(filepath)\n",
        "\n",
        "  if method == 'codon':\n",
        "    max_length = MAX_TOKENS * 3\n",
        "  elif method == 'kmer':\n",
        "    max_length = MAX_TOKENS - (k - 1)\n",
        "  elif method == 'bpe':\n",
        "    max_length = MAX_TOKENS\n",
        "\n",
        "  # Process data to get sequences of appropriate length \n",
        "  df = preprocess_data(filepath, max_length)\n",
        "  sequences = df['sequence'].values.tolist()\n",
        "\n",
        "  if method == 'bpe':\n",
        "    train_bpe_tokenizer(sequences)           # FOR NOW -- TRAIN TOKENIZER FOR BPE\n",
        "\n",
        "  # Tokenize according to chosen method\n",
        "  for seq in range(len(sequences)):\n",
        "    if method == 'codon':\n",
        "      tokens.append(seq2codon(sequences[seq]))\n",
        "    elif method == 'kmer':\n",
        "      tokens.append(seq2kmer(sequences[seq], k))\n",
        "    elif method == 'bpe':\n",
        "      tokens.append(seq2bpe(sequences[seq]))\n",
        "  df['tokenized'] = tokens\n",
        "  df['label'] = [label] * len(tokens)\n",
        "  \n",
        "  # Shuffle and save to csv\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  write_csv(filename, df)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Read fasta file and truncate sequences to appropriate length, returns dataframe\n",
        "\n",
        "Input:\n",
        "  file -- str, path to fasta file\n",
        "  max_length -- int, maximum sequence length\n",
        "\n",
        "Returns:\n",
        "  df -- dataframe, includes the > input line, start position, and sequence\n",
        "\"\"\" \n",
        "def preprocess_data(file, max_length): \n",
        "  records = []\n",
        "  for record in SeqIO.parse(file, 'fasta'):\n",
        "    name = str(record.name)\n",
        "    seq = str(record.seq).upper()\n",
        "    pos = 0 \n",
        "    # Truncate sequences if longer than max_length\n",
        "    while len(seq) > max_length:\n",
        "      records.append(                  # add subsequence up to max_length\n",
        "        {\n",
        "          'name': name,\n",
        "          'start': pos,\n",
        "          'sequence': seq[:max_length]\n",
        "        }\n",
        "      )\n",
        "      seq = seq[max_length:]           # sequence continuing from max_length\n",
        "      pos += max_length\n",
        "    records.append(\n",
        "        {\n",
        "          'name': name,\n",
        "          'start': pos,\n",
        "          'sequence': seq\n",
        "        }\n",
        "    )\n",
        "  df = pd.DataFrame(data=records)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Read in sequences and tokens to attach labels and return dataframe\n",
        "\n",
        "Input:\n",
        "  sequences -- list, original sequences\n",
        "  tokens -- list, tokenized sequences\n",
        "  label -- int, 1 for phage or 0 for bacteria\n",
        "\n",
        "Returns:\n",
        "  df -- dataframe\n",
        "\"\"\" \n",
        "def attach_labels(sequences, tokens, label):\n",
        "  d = []\n",
        "  for i in range(len(tokens)):\n",
        "    d.append(\n",
        "        {\n",
        "          'sequence': sequences[i],\n",
        "          'tokenized': tokens[i],\n",
        "          'label': label\n",
        "        }\n",
        "    )\n",
        "  df = pd.DataFrame(data=d)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Save the given dataframe to two separate csv files:\n",
        "1. full_output.csv includes the name, start position, sequence, tokenized\n",
        "   sequence, and label.\n",
        "2. tokenized_output.csv includes the tokenized sequence and the label.\n",
        "\n",
        "Input:\n",
        "  df -- dataframe, full dataframe of tokenized sequences\n",
        "\"\"\" \n",
        "def write_csv(filename, df):\n",
        "  df.to_csv(filename + '_full_output.csv', encoding='utf-8', index=False)\n",
        "  files.download(filename + '_full_output.csv')\n",
        "\n",
        "  tokenized = df[['tokenized', 'label']]\n",
        "  tokenized.to_csv(filename + '_tokenized_output.csv', encoding='utf-8', index=False)\n",
        "  files.download(filename + '_tokenized_output.csv')\n",
        "\n",
        "## Different tokenization methods\n",
        "\n",
        "\"\"\"\\\n",
        "Convert a sequence to codons\n",
        "\n",
        "Input:\n",
        "  seq -- str, original sequence\n",
        "\n",
        "Returns:\n",
        "  codons -- str, codons separated by space\n",
        "\"\"\"\n",
        "def seq2codon(seq):\n",
        "  codon = [seq[i:i+3] for i in range(0,len(seq),3)]\n",
        "  codons = \" \".join(codon)\n",
        "  return codons\n",
        "\n",
        "\"\"\"\\\n",
        "Convert a sequence to kmers\n",
        "\n",
        "Input:\n",
        "  seq -- str, original sequence\n",
        "  k -- int, kmer of length k\n",
        "\n",
        "Returns:\n",
        "  kmers -- str, kmers separated by space\n",
        "\"\"\"\n",
        "def seq2kmer(seq, k):\n",
        "  kmer = [seq[i:i+k] for i in range(len(seq)+1-k)]\n",
        "  kmers = \" \".join(kmer)\n",
        "  return kmers"
      ],
      "metadata": {
        "id": "gdxyb4KhfFz9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  TODO: byte pair encoding\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, normalizers\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def train_bpe_tokenizer(sequences):\n",
        "  # Initialize the tokenizer\n",
        "  tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "  # Customize the tokenizer to handle DNA sequences\n",
        "  tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
        "\n",
        "  # Train the tokenizer on your DNA sequences\n",
        "  trainer = trainers.BpeTrainer(vocab_size=1000)\n",
        "  tokenizer.train_from_iterator(sequences, trainer=trainer)\n",
        "\n",
        "  # Save the trained tokenizer\n",
        "  tokenizer.save(\"dna_tokenizer.json\")\n",
        "\n",
        "def seq2bpe(sequence):                          # RIGHT NOW, just doing the full sequence. probably want to split it\n",
        "  # Load the trained tokenizer \n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"dna_tokenizer.json\")\n",
        "\n",
        "  # Now, use the tokenizer object to tokenize your DNA sequences\n",
        "  encoded_input = tokenizer(sequence, return_tensors=\"pt\")\n",
        " \n",
        "  # Get the tokenized sequence\n",
        "  token_ids = encoded_input.input_ids\n",
        "  return tokenizer.batch_decode(token_ids)"
      ],
      "metadata": {
        "id": "HBcQTnlC5d45"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test\n",
        "tokenize(\"/content/sample_data/GCF_022922415.1_ASM2292241v1_cds_from_genomic.fna\", 0, \"bpe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "ipeNHcoiwHuc",
        "outputId": "75228bf3-c52a-4a51-f179-30b259b416a4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f4d81e44-745c-4ba4-9ecf-9a42eac99932\", \"GCF_022922415.1_ASM2292241v1_cds_from_genomic.fna_full_output.csv\", 3402818)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_98aa699b-9eb1-46b9-a5c8-d2c6bc46aeda\", \"GCF_022922415.1_ASM2292241v1_cds_from_genomic.fna_tokenized_output.csv\", 1839857)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           name  start  \\\n",
              "0      lcl|NZ_CP094094.1_cds_WP_080131717.1_603      0   \n",
              "1       lcl|NZ_CP094094.1_cds_WP_245066239.1_70      0   \n",
              "2       lcl|NZ_CP094094.1_cds_WP_245066250.1_77      0   \n",
              "3      lcl|NZ_CP094094.1_cds_WP_245066901.1_631      0   \n",
              "4      lcl|NZ_CP094094.1_cds_WP_245065555.1_813      0   \n",
              "...                                         ...    ...   \n",
              "1539  lcl|NZ_CP094094.1_cds_WP_245065929.1_1132      0   \n",
              "1540   lcl|NZ_CP094094.1_cds_WP_245065656.1_901      0   \n",
              "1541   lcl|NZ_CP094094.1_cds_WP_245066490.1_272      0   \n",
              "1542   lcl|NZ_CP094094.1_cds_WP_245066469.1_259      0   \n",
              "1543   lcl|NZ_CP094094.1_cds_WP_017180719.1_478      0   \n",
              "\n",
              "                                               sequence  \\\n",
              "0     ATGTCTTTACTTTCTAATCCATTCTTTCATTCGCTCTTCTCCATCA...   \n",
              "1     ATGCAAGATTTTATCAATGGATTTTTAAAGGCGTGGAAAGCTTGGA...   \n",
              "2     ATGAATAACAGCGTCATCATTATTGAAAGTCCTAATAAGGTAGCTA...   \n",
              "3     ATGGGAAAATTTTCTAAATTAGGCTTTATTTTAGCCACTTTAGGTA...   \n",
              "4     ATGCCATTTGAAGCTGTAATCGGGCTAGAAGTCCATGTCCAACTCA...   \n",
              "...                                                 ...   \n",
              "1539  ATGCAAAAAAATATATTAAAAATGACTCTGTTGTTGGTTTTCCTCT...   \n",
              "1540  GTGAAAAAAATCGTTGTGAGTTGGTGTGTGGCGTTGGCTTTTTTAA...   \n",
              "1541  ATGAAAAAATTGGTTTTAATCATTTTTTTAACGCTAACACTTTCAA...   \n",
              "1542  ATGGCTAAAGAAAATCTGCCTGTCGTTTTTGGGCCTGTTTTATCCA...   \n",
              "1543  TTGAGAACCTTGTTAAAAATGTTAGTTGGCACAAGCTTGCTGACAC...   \n",
              "\n",
              "                                              tokenized  label  \n",
              "0     [A TGTC TTTA CTT TCTAA TCCATT CTT TCATT CGC TC...      0  \n",
              "1     [A TGCAAGA TTTTA TCAA TGGATTTT TAAAGGC GTGG AA...      0  \n",
              "2     [ATGAA TAA CAGC GTCA TCA TTATT GAAA GTC CTAA T...      0  \n",
              "3     [ATGG GAAAA TTTT CTAAA TTA GGCTTTA TTTTA GCCA ...      0  \n",
              "4     [ATGC CATT TGAAGC TGTAA TC GGGC TAGAA GTCCA TG...      0  \n",
              "...                                                 ...    ...  \n",
              "1539  [ATGC AAAAAAA TATA TTAAAAA TGA CTC TGTT GTT GG...      0  \n",
              "1540  [GTG AAAAAAA TCGTT GTGA GTTGG TGTG TGGCGTT GGC...      0  \n",
              "1541  [A TGAAAAAA TT GG TTTTAA TCA TTTTTT TAACGC TAA...      0  \n",
              "1542  [ATGGC TAAAGAAAA TC TGCC TGTC GTTTT TGG GCC TG...      0  \n",
              "1543  [TTGA GAA CCTT GTTAAAAA TGTTA GTT GGCA CAAGCTT...      0  \n",
              "\n",
              "[1544 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7092e1ca-65e3-48e3-ad67-e0e254aaf9af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>start</th>\n",
              "      <th>sequence</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_080131717.1_603</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGTCTTTACTTTCTAATCCATTCTTTCATTCGCTCTTCTCCATCA...</td>\n",
              "      <td>[A TGTC TTTA CTT TCTAA TCCATT CTT TCATT CGC TC...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245066239.1_70</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGCAAGATTTTATCAATGGATTTTTAAAGGCGTGGAAAGCTTGGA...</td>\n",
              "      <td>[A TGCAAGA TTTTA TCAA TGGATTTT TAAAGGC GTGG AA...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245066250.1_77</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGAATAACAGCGTCATCATTATTGAAAGTCCTAATAAGGTAGCTA...</td>\n",
              "      <td>[ATGAA TAA CAGC GTCA TCA TTATT GAAA GTC CTAA T...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245066901.1_631</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGGGAAAATTTTCTAAATTAGGCTTTATTTTAGCCACTTTAGGTA...</td>\n",
              "      <td>[ATGG GAAAA TTTT CTAAA TTA GGCTTTA TTTTA GCCA ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245065555.1_813</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGCCATTTGAAGCTGTAATCGGGCTAGAAGTCCATGTCCAACTCA...</td>\n",
              "      <td>[ATGC CATT TGAAGC TGTAA TC GGGC TAGAA GTCCA TG...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1539</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245065929.1_1132</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGCAAAAAAATATATTAAAAATGACTCTGTTGTTGGTTTTCCTCT...</td>\n",
              "      <td>[ATGC AAAAAAA TATA TTAAAAA TGA CTC TGTT GTT GG...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1540</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245065656.1_901</td>\n",
              "      <td>0</td>\n",
              "      <td>GTGAAAAAAATCGTTGTGAGTTGGTGTGTGGCGTTGGCTTTTTTAA...</td>\n",
              "      <td>[GTG AAAAAAA TCGTT GTGA GTTGG TGTG TGGCGTT GGC...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1541</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245066490.1_272</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGAAAAAATTGGTTTTAATCATTTTTTTAACGCTAACACTTTCAA...</td>\n",
              "      <td>[A TGAAAAAA TT GG TTTTAA TCA TTTTTT TAACGC TAA...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_245066469.1_259</td>\n",
              "      <td>0</td>\n",
              "      <td>ATGGCTAAAGAAAATCTGCCTGTCGTTTTTGGGCCTGTTTTATCCA...</td>\n",
              "      <td>[ATGGC TAAAGAAAA TC TGCC TGTC GTTTT TGG GCC TG...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1543</th>\n",
              "      <td>lcl|NZ_CP094094.1_cds_WP_017180719.1_478</td>\n",
              "      <td>0</td>\n",
              "      <td>TTGAGAACCTTGTTAAAAATGTTAGTTGGCACAAGCTTGCTGACAC...</td>\n",
              "      <td>[TTGA GAA CCTT GTTAAAAA TGTTA GTT GGCA CAAGCTT...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1544 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7092e1ca-65e3-48e3-ad67-e0e254aaf9af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7092e1ca-65e3-48e3-ad67-e0e254aaf9af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7092e1ca-65e3-48e3-ad67-e0e254aaf9af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "#\n",
        "# def seq2bpe(sequence):\n",
        "#   splitted = []\n",
        "#   prev = 0\n",
        "#   while True:\n",
        "#     n = random.randint(1,3)\n",
        "#     splitted.append(sequence[prev:prev+n])\n",
        "#     prev = prev + n\n",
        "#     if prev >= len(sequence)-1:\n",
        "#         break\n",
        "#   return splitted"
      ],
      "metadata": {
        "id": "BuGLaGlWD8Mx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}