{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/tokenization/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vzDUnVBe3Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf9d5ec-b125-46d9-ec7b-712102b35342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Install Biopython \n",
        "try:\n",
        "    import google.colab\n",
        "    # Running on Google Colab, so install Biopython first\n",
        "    !pip install biopython\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkZE4bpvxM2E",
        "outputId": "be0327c8-a402-4a35-871b-9222e2f2407f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resources:\n",
        "# https://www.tutorialspoint.com/biopython/biopython_sequence_io_operations.htm\n",
        "# https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n",
        "# https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
      ],
      "metadata": {
        "id": "8nfx8YAcxSll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from Bio import SeqIO\n",
        "from google.colab import files\n",
        "\n",
        "MAX_TOKENS = 510\n",
        "\n",
        "## Given a phage directory and a bacteria directory, tokenize all fasta files according to method of choice \n",
        "\n",
        "def read_files(phage_dir, bacteria_dir, method, *args, **kwargs):\n",
        "  k = kwargs.get('k', None)\n",
        "  for file in os.listdir(phage_dir):\n",
        "    f = os.path.join(phage_dir, file)\n",
        "    if os.path.isfile(f):\n",
        "      tokenize(f, 1, method, k)\n",
        "  for file in os.listdir(bacteria_dir):\n",
        "    f = os.path.join(bacteria_dir, file)\n",
        "    if os.path.isfile(f):\n",
        "      tokenize(f, 0, method, k)\n",
        "\n",
        "## Tokenizes a sequence given a fasta file and max length\n",
        "\n",
        "\"\"\"\\\n",
        "Runs fasta files through tokenizer and adds the label of 1 for phage and\n",
        "0 for bacteria. Then shuffles the rows in the dataframe and saves to CSV \n",
        "\n",
        "Input:\n",
        "  phage -- str, path to phage fasta file\n",
        "  bacteria -- str, path to bacteria fasta file\n",
        "  method -- str, tokenization method of choice\n",
        "  k -- int, length of k if using kmer tokenization\n",
        "\"\"\"\n",
        "def tokenize(file, label, method, *args, **kwargs):\n",
        "  sequences = []\n",
        "  tokens = []\n",
        "  \n",
        "  k = kwargs.get('k', None)\n",
        "\n",
        "  if method == 'codon':\n",
        "    max_length = MAX_TOKENS * 3\n",
        "  elif method == 'kmer':\n",
        "    max_length = MAX_TOKENS - (k - 1)\n",
        "\n",
        "  # Process data to get sequences of appropriate length \n",
        "  df = preprocess_data(file, max_length)\n",
        "  sequences = df[\"sequence\"].values.tolist()\n",
        "\n",
        "  # Tokenize according to chosen method\n",
        "  for seq in range(len(sequences)):\n",
        "    if method == 'codon':\n",
        "      tokens.append(seq2codon(sequences[seq]))\n",
        "    elif method == 'kmer':\n",
        "      tokens.append(seq2kmer(sequences[seq], k))\n",
        "  df[\"tokenized\"] = tokens\n",
        "  df[\"label\"] = [label] * len(tokens)\n",
        "  \n",
        "  # Shuffle and save to csv\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  write_csv(df)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Read fasta file and truncate sequences to appropriate length, returns dataframe\n",
        "\n",
        "Input:\n",
        "  file -- str, path to fasta file\n",
        "  max_length -- int, maximum sequence length\n",
        "\n",
        "Returns:\n",
        "  df -- dataframe, includes the > input line, start position, and sequence\n",
        "\"\"\" \n",
        "def preprocess_data(file, max_length): \n",
        "  records = []\n",
        "  for record in SeqIO.parse(file, 'fasta'):\n",
        "    name = str(record.name)\n",
        "    seq = str(record.seq).upper()\n",
        "    pos = 0 \n",
        "    # Truncate sequences if longer than max_length\n",
        "    while len(seq) > max_length:\n",
        "      records.append(                  # add subsequence up to max_length\n",
        "        {\n",
        "          'name': name,\n",
        "          'start': pos,\n",
        "          'sequence': seq[:max_length]\n",
        "        }\n",
        "      )\n",
        "      seq = seq[max_length:]           # sequence continuing from max_length\n",
        "      pos = pos + max_length\n",
        "    records.append(\n",
        "        {\n",
        "          'name': name,\n",
        "          'start': pos,\n",
        "          'sequence': seq\n",
        "        }\n",
        "    )\n",
        "  df = pd.DataFrame(data=records)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Read in sequences and tokens to attach labels and return dataframe\n",
        "\n",
        "Input:\n",
        "  sequences -- list, original sequences\n",
        "  tokens -- list, tokenized sequences\n",
        "  label -- int, 1 for phage or 0 for bacteria\n",
        "\n",
        "Returns:\n",
        "  df -- dataframe\n",
        "\"\"\" \n",
        "def attach_labels(sequences, tokens, label):\n",
        "  d = []\n",
        "  for i in range(len(tokens)):\n",
        "    d.append(\n",
        "        {\n",
        "          'sequence': sequences[i],\n",
        "          'tokenized': tokens[i],\n",
        "          'label': label\n",
        "        }\n",
        "    )\n",
        "  df = pd.DataFrame(data=d)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Save the given dataframe to two separate csv files:\n",
        "1. full_output.csv includes the name, start position, sequence, tokenized\n",
        "   sequence, and label.\n",
        "2. tokenized_output.csv includes the tokenized sequence and the label.\n",
        "\n",
        "Input:\n",
        "  df -- dataframe, full dataframe of tokenized sequences\n",
        "\"\"\" \n",
        "def write_csv(df):\n",
        "  df.to_csv('full_output.csv', encoding='utf-8', index=False)\n",
        "  files.download('full_output.csv')\n",
        "\n",
        "  tokenized = df[['tokenized', 'label']]\n",
        "  tokenized.to_csv('tokenized_output.csv', encoding='utf-8', index=False)\n",
        "  files.download('tokenized_output.csv')\n",
        "\n",
        "## Different tokenization methods\n",
        "\n",
        "\"\"\"\\\n",
        "Convert a sequence to codons\n",
        "\n",
        "Input:\n",
        "  seq -- str, original sequence\n",
        "\n",
        "Returns:\n",
        "  codons -- str, codons separated by space\n",
        "\"\"\"\n",
        "def seq2codon(seq):\n",
        "  codon = [seq[i:i+3] for i in range(0,len(seq),3)]\n",
        "  codons = \" \".join(codon)\n",
        "  return codons\n",
        "\n",
        "\"\"\"\\\n",
        "Convert a sequence to kmers\n",
        "\n",
        "Input:\n",
        "  seq -- str, original sequence\n",
        "  k -- int, kmer of length k\n",
        "\n",
        "Returns:\n",
        "  kmers -- str, kmers separated by space\n",
        "\"\"\"\n",
        "def seq2kmer(seq, k):\n",
        "  kmer = [seq[i:i+k] for i in range(len(seq)+1-k)]\n",
        "  kmers = \" \".join(kmer)\n",
        "  return kmers\n",
        "\n",
        "#  TODO: byte pair encoding"
      ],
      "metadata": {
        "id": "gdxyb4KhfFz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test\n",
        "tokenize(\"/content/sample_data/GCF_022922415.1_ASM2292241v1_cds_from_genomic.fna\", 0, \"codon\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ipeNHcoiwHuc",
        "outputId": "dc667c57-fa79-4d22-bb7e-1500e04c2a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c971cfb9-8a50-4b99-8393-62a98c31f6ec\", \"full_output.csv\", 3572020)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aeabe88f-137a-4ceb-9f53-3ec57d7d9d5c\", \"tokenized_output.csv\", 1998181)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## OLD TESTING\n",
        "\n",
        "# # Swap between tokenization methods (used for testing)\n",
        "# def tokenize_sequences(file_path, method, *args, **kwargs):\n",
        "#   sequences = []\n",
        "#   tokens = []\n",
        "\n",
        "#   k = kwargs.get('k', None)\n",
        "\n",
        "#   if method == 'codon':\n",
        "#     max_length = 512*3\n",
        "#     sequences = preprocess_data(file_path, max_length)\n",
        "#     for i in range(len(sequences)):\n",
        "#       tokens.append(seq2codon(sequences[i]))\n",
        "  \n",
        "#   elif method == 'kmer':\n",
        "#     max_length = 512\n",
        "#     sequences = preprocess_data(file_path, max_length)\n",
        "#     for i in range(len(sequences)):\n",
        "#       tokens.append(seq2kmer(sequences[i], k))\n",
        "\n",
        "#   # Get output\n",
        "#   output = attach_labels(sequences, tokens, 0)\n",
        "#   return output\n",
        "\n",
        "# # Test \n",
        "# tokenize_sequences(\"/content/sample_data/GCF_022922415.1_ASM2292241v1_cds_from_genomic.fna\", 'codon')"
      ],
      "metadata": {
        "id": "pNDV7yxrG1MV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}