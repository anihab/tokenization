{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/tokenization/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vzDUnVBe3Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d02d2e0-4f1e-4734-f277-47f05335b687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Install Biopython \n",
        "try:\n",
        "    import google.colab\n",
        "    # Running on Google Colab, so install Biopython first\n",
        "    !pip install biopython\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkZE4bpvxM2E",
        "outputId": "1741514a-f614-474c-8171-9e8d806f93cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resources:\n",
        "# https://www.tutorialspoint.com/biopython/biopython_sequence_io_operations.htm\n",
        "# https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n",
        "# https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
      ],
      "metadata": {
        "id": "8nfx8YAcxSll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenizes a sequence given a fasta file and max length\n",
        "from Bio import SeqIO\n",
        "from google.colab import files\n",
        "\n",
        "\"\"\"\\\n",
        "Read fasta file and truncate sequences to appropriate length, returns dataframe\n",
        "\n",
        "Input:\n",
        "  file -- str, path to fasta file\n",
        "  max_length -- int, maximum sequence length\n",
        "\n",
        "Returns:\n",
        "  df -- dataframe, includes the > input line, start position, and sequence\n",
        "\"\"\" \n",
        "def preprocess_data(file, max_length): \n",
        "  records = []\n",
        "  for record in SeqIO.parse(file, 'fasta'):\n",
        "    name = str(record.name)\n",
        "    seq = str(record.seq).upper() \n",
        "    # Truncate sequences if longer than max_length\n",
        "    while len(seq) > max_length:\n",
        "      records.append(                  # add subsequence up to max_length\n",
        "        {\n",
        "          'name': name,\n",
        "          'start': 0, # TODO\n",
        "          'sequence': seq[:max_length]\n",
        "        }\n",
        "      )\n",
        "      seq = seq[max_length:]           # sequence continuing from max_length\n",
        "    records.append(\n",
        "        {\n",
        "          'name': name,\n",
        "          'start': 0, # TODO\n",
        "          'sequence': seq\n",
        "        }\n",
        "    )\n",
        "  df = pd.DataFrame(data=records)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Read in sequences and tokens to attach labels and return dataframe\n",
        "\n",
        "Input:\n",
        "  sequences -- list, original sequences\n",
        "  tokens -- list, tokenized sequences\n",
        "  label -- int, 1 for phage or 0 for bacteria\n",
        "\n",
        "Returns:\n",
        "  df -- dataframe\n",
        "\"\"\" \n",
        "def attach_labels(sequences, tokens, label):\n",
        "  d = []\n",
        "  for i in range(len(tokens)):\n",
        "    d.append(\n",
        "        {\n",
        "          'sequence': sequences[i],\n",
        "          'tokenized': tokens[i],\n",
        "          'label': label\n",
        "        }\n",
        "    )\n",
        "  df = pd.DataFrame(data=d)\n",
        "  return df\n",
        "\n",
        "\"\"\"\\\n",
        "Save the given dataframe to two separate csv files:\n",
        "1. full_output.csv includes the name, start position, sequence, tokenized\n",
        "   sequence, and label.\n",
        "2. tokenized_output.csv includes the tokenized sequence and the label.\n",
        "\n",
        "Input:\n",
        "  df -- dataframe, full dataframe of tokenized sequences\n",
        "\"\"\" \n",
        "def write_csv(df):\n",
        "  df.to_csv('full_output.csv', encoding='utf-8', index=False)\n",
        "  files.download('full_output.csv')\n",
        "\n",
        "  tokenized = df[['tokenized', 'label']]\n",
        "  tokenized.to_csv('tokenized_output.csv', encoding='utf-8', index=False)\n",
        "  files.download('tokenized_output.csv')\n",
        "\n",
        "## Different tokenization methods\n",
        "\n",
        "\"\"\"\\\n",
        "Convert a sequence to codons\n",
        "\n",
        "Input:\n",
        "  seq -- str, original sequence\n",
        "\n",
        "Returns:\n",
        "  codons -- str, codons separated by space\n",
        "\"\"\"\n",
        "def seq2codon(seq):\n",
        "  codon = [seq[i:i+3] for i in range(0,len(seq),3)]\n",
        "  codons = \" \".join(codon)\n",
        "  return codons\n",
        "\n",
        "\"\"\"\\\n",
        "Convert a sequence to kmers\n",
        "\n",
        "Input:\n",
        "  seq -- str, original sequence\n",
        "  k -- int, kmer of length k\n",
        "\n",
        "Returns:\n",
        "  kmers -- str, kmers separated by space\n",
        "\"\"\"\n",
        "def seq2kmer(seq, k):\n",
        "  kmer = [seq[i:i+k] for i in range(len(seq)+1-k)]\n",
        "  kmers = \" \".join(kmer)\n",
        "  return kmers\n",
        "\n",
        "#  TODO: byte pair encoding"
      ],
      "metadata": {
        "id": "gdxyb4KhfFz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MAX_TOKENS = 510\n",
        "\n",
        "# Takes in a phage directory and bacteria directory, and runs all of the files through a tokenizer\n",
        "def read_files(phage_dir, bacteria_dir):\n",
        "  for filename in os.listdir(phage_dir)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\\\n",
        "Runs fasta files through tokenizer and adds the label of 1 for phage and\n",
        "0 for bacteria. Then shuffles the rows in the dataframe and saves to CSV \n",
        "\n",
        "Input:\n",
        "  phage -- str, path to phage fasta file\n",
        "  bacteria -- str, path to bacteria fasta file\n",
        "  method -- str, tokenization method of choice\n",
        "  k -- int, length of kmer if using kmer tokenization\n",
        "\"\"\"\n",
        "def tokenize(phage, bacteria, method, *args, **kwargs):\n",
        "  phage_seq = []\n",
        "  phage_tokens = []\n",
        "  bacteria_seq = []\n",
        "  bacteria_tokens = []\n",
        "  \n",
        "  k = kwargs.get('k', None)\n",
        "\n",
        "  if method == 'codon':\n",
        "    max_length = MAX_TOKENS * 3\n",
        "  elif method == 'kmer':\n",
        "    max_length = MAX_TOKENS - (k - 1)\n",
        "\n",
        "  # Process data to get sequences of appropriate length \n",
        "  phage_df = preprocess_data(phage, max_length)\n",
        "  bacteria_df = preprocess_data(bacteria, max_length)\n",
        "\n",
        "  phage_seq = phage_df[\"sequence\"].values.tolist()\n",
        "  bacteria_seq = bacteria_df[\"sequence\"].values.tolist()\n",
        "\n",
        "  # Tokenize according to chosen method\n",
        "  for i in range(len(phage_seq)):\n",
        "    if method == 'codon':\n",
        "      phage_tokens.append(seq2codon(phage_seq[i]))\n",
        "    elif method == 'kmer':\n",
        "      phage_tokens.append(seq2kmer(phage_seq[i], k))\n",
        "  phage_df[\"tokenized\"] = phage_tokens\n",
        "  phage_df[\"label\"] = [1] * len(phage_tokens)\n",
        "      \n",
        "  for i in range(len(bacteria_seq)):\n",
        "    if method == 'codon':\n",
        "      bacteria_tokens.append(seq2codon(bacteria_seq[i]))\n",
        "    elif method == 'kmer':\n",
        "      bacteria_tokens.append(seq2kmer(bacteria_seq[i], k))\n",
        "  bacteria_df[\"tokenized\"] = bacteria_tokens\n",
        "  bacteria_df[\"label\"] = [0] * len(bacteria_tokens)\n",
        "  \n",
        "  # Shuffle and save to csv\n",
        "  frames = [phage_df, bacteria_df]\n",
        "  full_df = pd.concat(frames)\n",
        "  full_df = full_df.sample(frac=1).reset_index(drop=True)\n",
        "  write_csv(full_df)"
      ],
      "metadata": {
        "id": "RSAgsGn-2932"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## OLD TESTING\n",
        "\n",
        "# # Swap between tokenization methods (used for testing)\n",
        "# def tokenize_sequences(file_path, method, *args, **kwargs):\n",
        "#   sequences = []\n",
        "#   tokens = []\n",
        "\n",
        "#   k = kwargs.get('k', None)\n",
        "\n",
        "#   if method == 'codon':\n",
        "#     max_length = 512*3\n",
        "#     sequences = preprocess_data(file_path, max_length)\n",
        "#     for i in range(len(sequences)):\n",
        "#       tokens.append(seq2codon(sequences[i]))\n",
        "  \n",
        "#   elif method == 'kmer':\n",
        "#     max_length = 512\n",
        "#     sequences = preprocess_data(file_path, max_length)\n",
        "#     for i in range(len(sequences)):\n",
        "#       tokens.append(seq2kmer(sequences[i], k))\n",
        "\n",
        "#   # Get output\n",
        "#   output = attach_labels(sequences, tokens, 0)\n",
        "#   return output\n",
        "\n",
        "# # Test \n",
        "# tokenize_sequences(\"/content/sample_data/GCF_022922415.1_ASM2292241v1_cds_from_genomic.fna\", 'codon')"
      ],
      "metadata": {
        "id": "pNDV7yxrG1MV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}